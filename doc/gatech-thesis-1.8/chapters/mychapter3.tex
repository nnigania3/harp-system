\setcounter{equation}{0}

\chapter{System Design}
\label{chap: System Design}

\section{System Components}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{HARP ISA}
The ISA used as part of this work was a custom RISC-based ISA. The ISA was developed under the name \textbf{HARP} which stands for Heterogeneous Architecture Research Project. %\cite{schreier}. 
The main features of the ISA are: full predication, SIMD support, and customizability. Many features of this ISA are customizable like the vector width, instruction length, number of general-purpose and predicate registers etc. The main reason to do this was to add support for new instructions for future architectures and also to allow adapting the ISA quickly to various architectural configurations. There are several types of instructions, depending on the number of arguments (register, immediate, predicate registers etc.), but all instructions are encoded in a similar fashion. That is, the most significant bit of each instruction indicates if it's predicate or not, the next field specifies the predicate register, the next field stands for the opcode, and so on. 
%More details can be found in \cite{schreier}.

The HARP ISA is also supported by its tool chain called ``Harptool'' which acts as an assembler, linker, and emulator. For this work the benchmarks used were written directly in HARP Assembly, which is very similar to RISC-based assembly programs. These benchmarks, written with the aim for testing and performance purposes, are listed in Table~\ref{table:apps}

%Applications for 1 Lane HARP Core:
%\begin{itemize}
%\item Array Sum
%\item Sieve of Eratosthenes (finding prime numbers in a range of numbers)
%\item Bubble sort
%\item Matrix multiplication
%\end{itemize}
%
%SIMD apps:
%\begin{itemize}
%\item Array Sum (coalesced and un-coalesced version)
%\item Matrix multiplication
%\end{itemize}

\begin{table}[!htbp]
  \caption{Micro-Benchmarks studied.}
  \centering
  \begin{tabular}{|l|c|}
    \hline
    \multicolumn{2}{|c|}{Single Lane Benchmarks} \\
    \hline
Array Sum					&Sum 240 numbers \\
Sieve of Eratosthenes	&Finding prime numbers between 1 to 100\\
Bubble sort					&Sort Numbers 0-9 using bubble sort\\
Matrix multiplication	&Multiply two 8x8 matrices\\
    \hline
    \multicolumn{2}{|c|}{Multi-Lane Benchmarks} \\
    \hline
Array Sum					&Sum 240 numbers (coalesced and un-coalesced version) \\
Matrix multiplication	&Multiply two 8x8 matrices\\
    \hline
  \end{tabular}
  \label{table:apps}
\end{table}

We varied the input size (array size for array sum, matrix size for matrixmul, input range for sorting) and ran it on our design, which was used not only to test the overall design, but also to get an idea of the performance. These are very naive applications compared to some of the complex benchmarks that are available, but these do provide good credibility for the design if not the performance as of now. One of the main focuses of writing these applications was to stress corner cases. Part of the future work is to design a tool that can translate CUDA or OpenCL code to HARP Assembly, once that is in place it will allow us to do more thorough testing.

The HARP ISA in its current version alos supports only a single simple console I/O to help debug and display the output. Any store instruction at an address 0x80000000 causes the HARP core to send the required data to the display (7-segment display or LEDs or VGA).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Core}
\subsubsection {Core Pipeline}

The base HARP core design used as part of this work was a design written completely in CHLD. This section provides an overview of the base design of the core, which was modified for use as part of this work. The HARP core is an in-order issue and out-of-order completion pipelined processor. The pipeline stages are mainly Instruction Fetch, Decode/Register File access/Issue, Execute and WriteBack stage. The function of each stage is implied from its name and is shown in Figure~\ref{fig:harmonica_chad}. 
% LEFT, BOTTOM, RIGHT,TOP
\figput{harmonica_chad}{6.0in}{0.0in 0.0in 0.0in 0.0in}{HARP core pipeline.}

The instruction fetch stage reads the instruction stored in the instruction ROM and passes it to the next stage. To improve performance, there is also a GHB (global history based) branch predictor and a BTB (branch target buffer). The PHT (pattern history table) of the GHB and BTB is indexed by XORing the PC and the branch history. The target branch address is obtained from the BTB and the direction from the PHT (pattern history table). Next, the decode stage decodes the instruction and reads all the registers required for each instruction. It also checks for dependencies and stalls the pipeline if the registers it needs are not valid, as the responsible instructions have not updated them yet. To handle dependencies, the design assigns a unique Instruction ID (IID) to each instruction, which is used to determine which instruction is responsible for updating the register file to its latest value and set the valid bit. So, each instruction updates the IID file for the register it writes to with its own IID and also sets the valid bit to zero before proceeding to the execute stage. The predication logic is also implemented in the decode stage. We convert the instruction to an NOP, depending on the value of the predicate register. There is no support for data forwarding in this design right now; it has been left for future work. 

The execute stage has all the ALU units to implement the functions offered by the ISA. Additional modules like faster ALU units, say for example fast pipelined multiply, divide, or floating point operations, can be written separately in Verilog and integrated with our current design as and when needed. This stage also has the memory management unit (MMU) to handle memory accesses, which are discussed later. %, as we did not see a lot of need for a MMU or load/store queue for a single lane core.
The write-back stage writes back the updated data to the register files and updates the state of the registers (set the valid bit) so the dependent instructions can now progress.

Factors such as SIMD width, number of registers, ROM size, and data/address width can be varied. We discuss more about the SIMD support in the next part and later sections discuss more about the memory management unit. We also allow for a version of the core without any cache support but with each lane having a small RAM and a ROM on its own. This variation is just to explore different architecture possibilities.

\subsubsection {SIMD support}

Writing code in CHDL allowed us to easily extend our core design for SIMD support. The whole register file was instantiated as many times as the number of lanes, hence converting each register to a vector register. For a given vector instruction, the same operation was done for each word in the vector register. Once all the vector registers are read, they are sent to the execute stage. If the instruction involved an immediate, then the same immediate was used for all the lanes.

In the execution unit each of the ALU units was instantiated as many times as the number of lanes and their corresponding inputs were passed on from the decode stage. Everything else (checking for dependencies logic etc.) remained mostly the same as in the case of a single lane (branch outcomes, predication outputs were determined only by the first lane). As of now, the core does not support any mask operations, so the same operation is done on all the words of the vector register. Adding support for masking via mask registers and also support for branch divergence will be part of the future work. The only big change in order to support SIMD instructions was done in the memory management unit. To enable this, a complex load store queue, shown in Figure ~\ref{fig:mmu}, was designed and is discussed in the next section. Also, for SIMD support, changes had to be made in the cache, as the L1 cache now has to send and receive data at cache line granularity rather than word granularity; also the cache had to support masked writes in case of un-coalesced accesses.
% LEFT, BOTTOM, RIGHT,TOP
\figput{mmu}{5.0in}{0.0in 1.5in 0.0in 1.0in}{MMU or Load-Store unit interface.}

\subsubsection {Memory management unit (MMU)}

The whole memory management unit and the cache were among more complex portions of the design. This section discusses the memory management unit/load-store queue/coalescing unit used in the execute stage of the core to send requests to the cache subsystem.  The MMU can be divided into four major components, as seen in Figure ~\ref{fig:mmu2}:

\begin{itemize}
\item Front End
\item Load Queue
\item Store Queue
\item Cache Interface logic
\end{itemize}

We now briefly discuss the functionality of each of the components for an SIMD HARP core. This SIMD support requires the address/data granularity of a cache line between the MMU and cache. 
The design in Figure ~\ref{fig:mmu2} is an example of an 8-lane SIMD core along with an eight word cache line, so all lanes can together request one whole cache line every cycle. We also have a design variation without the MMU and blocking memory requests to save on logic resources.

% LEFT, BOTTOM, RIGHT,TOP
\figput{mmu2}{6.0in}{0.0in 1.5in 0.0in 1.0in}{MMU components.}

\noindent\textbf{Front End}: The front end receives the request from the core with addresses and data for each lane. It then tries to pack all lane requests made to the same cache line together and forms one request to be sent to the load/store queue. Hence, each entry in the load/store queue corresponds to a cache-line. For an un-coalesced request, the front end keeps creating as many new entries as unique cache lines. Along with the cache line address, each entry also keeps information about the lanes requesting or writing that cache line along with their corresponding word offsets in the cache line.

\noindent\textbf{Load Queue}: All load requests are passed on to this queue. The first request for an instruction is called the leader and all the following requests derived from the same instruction (in case of un-coalesced) are followers. The leader checks if data for all lanes are loaded by waiting until its followers return the required data. The follower writes the data it receives from the memory to the corresponding lane data slot in the leader entry. Once data for all lanes are loaded (indicated by a loaded bit for each lane), we mark the entry as done and ready to retire. The load requests can be serviced in any order. We also enable load store forwarding (LSF) feature where, if an incoming load request sees data for the corresponding cache line in the store queue, then it directly gets the data from the store queue entry and is marked as done. The current implementation is conservative in the sense, it gets data forwarded only when the whole cache line is being written by a store queue entry. If only a partial store is made to a cache line, we make the load entry wait until the store entry commits.

\noindent\textbf{Store Queue}: The store queue gets the store data along with the cache line address from the front end. It then checks if there is a pending load entry for the same address; if there is, then we make this request wait until the matching load request is serviced to handle WAR (write after read) hazard. Once the store request is ready to commit, we send the data along with the valid/mask bits for each word to the cache. When both load and store queues want to send their requests to the cache, we give higher preference to requests from the load queue.

\noindent\textbf{Cache Interface logic}: This part of the MMU takes each pending entry from the load or store queue and sends it to the cache. There is a lot of switching logic involved mainly to handle requests from the store queue entry. This is because we allow for cross lane stores wherein each lane can write to any word in a cache line. 
%except when they try to write the same word in the same line. 
We also have a broadcast logic built in, which allows multiple lanes in the same request to ask for any/same word in the cache line. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cache}
We use a two-level non-blocking, write-back, single-cycle latency cache for this design written completely in Verilog. A CHDL implementation was not written mainly because for the FPGA prototyping part we already have a base implementation of the cache design in Verilog and for the software simulation part we already have a cache design tightly integrated with our simulation infrastructure. We still have a CHDL design in the works. 
% LEFT, BOTTOM, RIGHT,TOP
 \figput{cache_block}{4.5in}{0.0in 1.5in 0.0in 1.0in}{Cache interface signals for core and memory sides.}
The input and output signals to the cache can be seen in Figure ~\ref{fig:cache_block}. The cache is parameterizable, where we can configure the cache line size (32 bytes used for our design), address/data width, and miss status handling register (MSHR) entries and also make data input/output granularity to be a word or cache line (for SIMD). Since we have designed a single-cycle latency cache, we need an extra 2x clock to allow us to perform tag read and write in the same clock cycle. This clock is fed in the top level system block and is generated via a phase locked loop (PLL). The L1 cache is a direct mapped cache and the L2 is 4-way set associative cache. We now discuss the main building blocks of the L1 cache; L2 is designed in a similar way except it uses a random cache line replacement policy and extra tag/data RAM modules for each extra way. Figure ~\ref{fig:cache_microarch} shows the internal design of the cache with each of the components briefly discussed below.

% LEFT, BOTTOM, RIGHT,TOP
 \figput{cache_microarch}{5.5in}{0.0in 0.2in 0.0in 0.2in}{Cache design.}
% \begin{itemize}

\textbf{Data/Tag ram}: These storage elements hold the data and tags along with dirty and valid bits. The actual data and tags are stored in eight two-ported 32-bit (word) RAMs (8x32 = 256 bit cache line). We need two ports ,as we need to service the request coming from the core as well as lower level memory (L2 cache). 

\textbf{MSHR} (Miss Status Handling Register): Since we implemented a non-blocking cache, we need an MSHR. This block stores the requests that miss in the cache and need to wait until the cache line it misses on is serviced by the lower level memory. This stores the data, addresses, read/write, and core request-id for each read/write operation. In the case, where a second request to the same waiting cache line occurs, we stall the cache and store the new request in a separate data structure. This new request is serviced only after the matching entry in the MSHR is serviced. A future extension to improve the performance will be to allow some kind of a piggy-back feature where, instead of stalling on the same cache line, we keep adding the requests to the same MSHR entry and service them all at once (though this will save us only a few cycles).

\textbf{Non-blocking FSM} (finite state machine): This state machine controls the overall operation of the cache. Since we are handling requests from the core and lower-level caches, the FSM keeps track of when a new request arrives from either side. Depending on the request, it issues control signals to the MSHR or the data/tag RAM to update their state or generates a stall signal to tell the core to stop sending more requests.

\textbf{Arbiter}: The arbiter is used for the multicore versions of our system, where each core has a private L1 cache and they share an L2 cache. The arbiter gets requests from L1 caches of all the cores and sends only one cache line request to the L2. It appends the core-id to the request-id coming from each core so it can return the data to the appropriate core when it is returned from the L2. Various arbitration schemes can be used, but for this work we used a priority encoder. The arbiter, like other components, is completely configurable allowing it to instantiate as many L1 caches as possible with only a small change in the Verilog code (more signals need to be added at the input port, as Verilog does not allow using a parameterized array of an input signal). %\TODO{many the priority encoder parameterizable}.
%\end{itemize}

To get a better understanding of how these blocks interact to service simple load and store requests, we discuss a few simple cases below.

\textbf{Read/Write Hit operation}: When a new request arrives in the cache, we read the tag RAM to see if there is a match (first cycle of the 2x clock); if there is a hit, we then send the data from the data RAM to the core in the next cycle along with asserting the valid out bit for a load request. For a store request we update our data and tags (second cycle of 2x clock) and don't need to reply anything to the core.

\textbf{Read/Write Miss operation}: In the case of a cache line miss, the FSM allocates a new entry in the MSHR for this request. If the L2 cache is not stalled, it sends the request to the L2 and waits for the data. The L1 cache in the meantime can receive new requests if there is free space left in the MSHR. Each cache miss is treated in a similar way unless the new request is for the same line waiting in the MSHR. In this case, we stall the L1 cache and wait until the matching MSHR entry is freed. Once the data from L2 arrives, it directly updates the cache data and tag RAM. Then, the FSM gets the MSRH entry responsible for the miss and issues it again, making it a cache hit this time. Once issued, the MSHR entry is freed and the data along with valid output is sent to the core.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Memory Controller}
The memory controller supported by the DE3 FPGA board used as part of this work is a DDR2 memory controller with an operating frequency ranging from 125 MHz - 533 MHz. We used a 1GB DDR2 RAM for this work running at 125MHz due to frequency limitations set by other components of the system. The DDR2 is connected on the board via 200 pins for clock and control signals coming from the FPGA. Figure ~\ref{fig:ddr_block} shows the top view of the memory controller block; more details can be found in \cite{ddr2}. The command generator receives the signals from the user side and passes it to the timing bank pool. The timing bank pool then checks for signal timings and if valid data is present in the data buffers. Then, it passes the request to the arbiter, which finally passes the command forward. The rank timer maintains rank-specific information to maintain correct functionality.

% LEFT, BOTTOM, RIGHT,TOP
 \figput{ddr_block}{5.0in}{0.0in 0.0in 0.0in 0.0in}{Test setup for memory controller IP and internal block diagram.}

The DDR2 controller IP was generated using Altera's MegaWizard \cite{quartus}, which automatically generates the IP along with some example files for simulation purposes. Although it might seem straightforward but creating this IP with the correct parameters set was critical. Many times, the RTL simulation (using a system Verilog model for a DDR2 memory) of a generated controller IP was found to be working, but it failed on the board because one of the timing parameters was set incorrectly. Even the timing presets present in the Altera tool for the actual DDR2 DIMM used in the board were not correct. The correct parameters were later obtained from an example DDR2 IP project from the FPGA board vendor (Terasic). Also, since the number of pins that need to be connected to the DDR2 was significant, they all had to be set very carefully, again using the parameter obtained from the board vendor, which was also a confusing part as Altera by default recommended another configuration. The incorrect pin and parameter configurations were the main reasons behind failures seen initially. We found that many DDR2 IP parameters like the frequency and the row open/close policy were configurable via Altera's IP generator. Though we can customize the IP RTL itself, we can't reuse all of Altera's propriety IPs. Different scheduling policies like FRFCFS etc. can be tried and can be one research direction going forward for different types of system architectures.

% LEFT, BOTTOM, RIGHT,TOP
 \figput{ddr_wave}{6.0in}{0.0in 0.5in 0.0in 0.5in}{DDR cache side signals for read/write operation \cite{ddr2}.}
Figure ~\ref{fig:ddr_wave} shows the timing diagrams for requests being sent to the memory controller. The user side signal uses Altera's Avlon interface \cite{ddr2}, and this is the prefix (`avl') given to these signals.
Before using the controller with the rest of the system, it had to be tested using a random traffic generator (generated along with IP). It does a thorough test to make sure the DDR2 runs for the parameters set on the board. We first tested the controller in RTL using a DDR2 memory model and then on the FPGA board. Once DDR2 was tested, we started integrating it with the rest of the system. Since the cache interface is slightly different from the user interface signals of the memory controller, a memory controller wrapper block was written to interface them. The main difference was that the cache identifies each request via a unique ID, whereas the DDR2 just returns the data along with valid bits on a first-come first-served basis, as can be seen in the DDR2 wave diagrams in Figure ~\ref{fig:ddr_wave}. This wrapper is mainly a FIFO-based structure that stores the request along with the ID of incoming request coming from the cache and sends the data returned from the main memory back to the cache along with the original request id.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Putting it together: A Multicore and multilane GP-GPU system}
Figure~\ref{fig:full_system} shows the final system that was designed and tested on the FPGA board. We saw in previous sections in this chapter how each of the components, namely the SIMD core, MMU, cache, and the memory controller, was designed. Now the aim of the work was to integrate each of these components to make the overall system. Although this might sound simple to start with, it was a fairly time-consuming task, as it was through this process that many hidden bugs were discovered. 
% LEFT, BOTTOM, RIGHT,TOP
\figput{full_system}{5.1in}{0.0in 0.0in 0.0in 0.3in}{Possible system designs.}

We started by testing each module before integrating them; this was done by writing separate testbenches (in Verilog) for each of the Verilog blocks. This led us to know if we were able to get the basic functionality out of the desired block. As many blocks were complex and had many inputs/outputs, it was hard to test them. Once basic building blocks were tested reasonably, we started integrating the whole system. To check overall functionality we started by testing a single lane HARP core system and made sure this was working and meeting all the timing constraints. For a system of this magnitude, it is generally hard to test all the corner cases and even now there might be some hidden bugs in the design. Since we tested our final system by writing applications rather than creating stimuli or a Verilog testbench, our tests were, to a large extent, thorough and helped us discover bugs in the very blocks which we had tested earlier individually.

The first step was to do RTL simulation of the whole system and make sure it was passing. Many functional bugs can be easily found and fixed at this stage. At the RTL stage, we can also replace each large component with a dummy model to isolate issues. Next, we synthesize and perform gate-level simulation; this is hard to debug, as signal names are changed or synthesized away and it takes a long time to reach the end of simulation. The best way to debug issues at this stage was to fix the warnings that were given by Altera's Quartus tool and this for us fixed many bugs in the design. Many issues, including the errors due to DDR2 timing issues, were hard to fix and took a lot of time. After this stage, we tested the system on the board by downloading the FPGA programming binary on the board. If it does not run even after this then the reasons we came across were mainly due to issues in the reset logic or the clock or the DDR timing parameters. One good practice is to use PLLs to generate cleaner clocks with less jitter. Most of the problems in my design were fixed using the above process. If one is still not able to isolate issues, then the next step is to use hardware debug IPs (Altera Signal Tap Analyzer), which can monitor signals on board and display them on the screen. This method was also used to debug some issues in the DDR2 as part of the test phase.

Once a single-lane version of the whole system was working on the board, we then tested a multi-lane version of the core using different tests to stress most of the commonly used blocks as shown in Figure~\ref{fig:full_system}. For example, we wrote test cases for doing a lot of un-coalesced, coalesced, and broadcast memory requests to determine whether or not the complex memory management unit was working properly. Once the single core (one lane or SIMD) was tested on the board, it was easy to create multi-core versions by instantiating more instances of the core with each core running its own separate application. Figure~\ref{fig:full_system} shows one such system; as can be seen we have two cores, each having its own private L1 cache and sharing an L2 cache. As mentioned previously, we don't have support for coherence as of now. The extra component needed to make this multicore system was the L1 cache arbitrator, which schedules requests from different cores to the L2. For this work, a priority arbiter was used, but other possibilities can also be explored.  To test it, we tried to run different combinations of applications on each of the cores accessing different data in the main memory. 

% LEFT, BOTTOM, RIGHT,TOP
\figput{biglittle}{5.0in}{0.0in 1.5in 0.0in 1.7in}{Heterogeneous system with one big and six little cores.}

After testing the above systems, we tried to create novel heterogeneous systems from our basic building blocks, as shown in Figure~\ref{fig:biglittle}. This was possible because of our tool chain and parameterized building blocks. We were able to test a heteregeneous system consisting of a complex big SIMD core and multiple smaller cores \cite{biglittle}. The bigger core has its own cache and memory subsystem (with the DDR) and the smaller cores have their own memory without any cache, hence consuming fewer FPGA resources. The results from the FPGA experiments for the systems above are discussed in the next section along with details of our heteregeneous system.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
